import os
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import numpy as np
from scipy.optimize import linear_sum_assignment
import logging
from tqdm import tqdm
import gymnasium as gym
from gymnasium.spaces import Box, MultiDiscrete
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
import matplotlib.pyplot as plt
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
from sb3_contrib import TRPO
from stable_baselines3.common.policies import ActorCriticPolicy
from functools import partial
import copy
import pickle
import cv2
from collections import deque
# Suppress potential warnings
warnings.filterwarnings("ignore", category=UserWarning, module="stable_baselines3.common.on_policy_algorithm")
# Configure detailed logging
logging.basicConfig(
    filename='marlmot_training_detailed.log',
    level=logging.INFO,  # INFO to reduce I/O overhead
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    filemode='w'
)
def compute_iou_vectorized(bboxes1, bboxes2):
    if len(bboxes1) == 0 or len(bboxes2) == 0:
        return np.zeros((len(bboxes1), len(bboxes2)))
    b1 = np.array(bboxes1)[:, np.newaxis]
    b2 = np.array(bboxes2)[np.newaxis, :]
    x_left = np.maximum(b1[..., 0], b2[..., 0])
    y_top = np.maximum(b1[..., 1], b2[..., 1])
    x_right = np.minimum(b1[..., 0] + b1[..., 2], b2[..., 0] + b2[..., 2])
    y_bottom = np.minimum(b1[..., 1] + b1[..., 3], b2[..., 1] + b2[..., 3])
    intersection = np.maximum(x_right - x_left, 0) * np.maximum(y_bottom - y_top, 0)
    union = b1[..., 2] * b1[..., 3] + b2[..., 2] * b2[..., 3] - intersection
    iou = np.where(union > 0, intersection / union, 0)
    return iou
class KalmanFilter:
    logger = logging.getLogger(__name__)
    def __init__(self, dim_x, dim_z):
        self.dim_x = dim_x
        self.dim_z = dim_z
        self.x = np.zeros(dim_x)
        self.P = np.eye(dim_x)
        self.F = np.eye(dim_x)
        self.H = np.zeros((dim_z, dim_x))
        self.Q = np.eye(dim_x)
        self.R = np.eye(dim_z)
        self.env = None
        self._init_kalman_filter()
    def predict(self):
        # logger.debug omitted for speed
        self.x = np.dot(self.F, self.x)
        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q
        self.x = np.nan_to_num(self.x, nan=0.0, posinf=1e6, neginf=-1e6)
        self.P = np.nan_to_num(self.P, nan=0.0, posinf=1e6, neginf=-1e6)
        self.x = np.clip(self.x, -1e6, 1e6)
        self.P = np.clip(self.P, 0, 1e6)
        # Apply CMC if available
        if self.env and hasattr(self.env, 'homographies') and self.env.frame_idx < len(self.env.homographies):
            hom = self.env.homographies[self.env.frame_idx]
            if not np.allclose(hom, np.eye(3)):
                cx, cy, w, h = self.x[0:4]
                corners = np.array([
                    [cx - w/2, cy - h/2, 1],
                    [cx + w/2, cy - h/2, 1],
                    [cx + w/2, cy + h/2, 1],
                    [cx - w/2, cy + h/2, 1]
                ]).T
                warped_corners = hom @ corners
                warped_corners /= warped_corners[2, :]
                min_x, min_y = np.min(warped_corners[0:2, :], axis=1)
                max_x, max_y = np.max(warped_corners[0:2, :], axis=1)
                new_w = max_x - min_x
                new_h = max_y - min_y
                new_cx = (min_x + max_x) / 2
                new_cy = (min_y + max_y) / 2
                self.x[0:4] = [new_cx, new_cy, new_w, new_h]
    def update(self, z):
        # logger.debug omitted for speed
        y = z - np.dot(self.H, self.x)
        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R
        try:
            K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))
        except np.linalg.LinAlgError:
            self.logger.warning("Singular matrix in Kalman update, resetting filter")
            self._init_kalman_filter()
            return
        self.x = self.x + np.dot(K, y)
        I_KH = np.eye(self.dim_x) - np.dot(K, self.H)
        self.P = np.dot(np.dot(I_KH, self.P), I_KH.T) + np.dot(np.dot(K, self.R), K.T)
        self.x = np.nan_to_num(self.x, nan=0.0, posinf=1e6, neginf=-1e6)
        self.P = np.nan_to_num(self.P, nan=0.0, posinf=1e6, neginf=-1e6)
        self.x = np.clip(self.x, -1e6, 1e6)
        self.P = np.clip(self.P, 0, 1e6)
    def _init_kalman_filter(self):
        self.F = np.array([
            [1, 0, 0, 0, 1, 0, 0, 0],
            [0, 1, 0, 0, 0, 1, 0, 0],
            [0, 0, 1, 0, 0, 0, 1, 0],
            [0, 0, 0, 1, 0, 0, 0, 1],
            [0, 0, 0, 0, 1, 0, 0, 0],
            [0, 0, 0, 0, 0, 1, 0, 0],
            [0, 0, 0, 0, 0, 0, 1, 0],
            [0, 0, 0, 0, 0, 0, 0, 1]
        ])
        self.H = np.array([
            [1, 0, 0, 0, 0, 0, 0, 0],
            [0, 1, 0, 0, 0, 0, 0, 0],
            [0, 0, 1, 0, 0, 0, 0, 0],
            [0, 0, 0, 1, 0, 0, 0, 0]
        ])
        self.P = np.eye(8) * 1000.0
        self.P[4:8, 4:8] *= 10.0
        self.Q = np.diag([0.1, 0.1, 0.01, 0.01, 1.0, 1.0, 0.1, 0.1])
        self.R = np.diag([10.0, 10.0, 5.0, 5.0])
class Agent:
    logger = logging.getLogger(__name__)
    def __init__(self, env):
        self.env = env
        self.bbox = None
        self.mode = 'inactive'
        self.track_id = None
        self.n_unassoc = 0
        self.n_streak = 0
        self.confidence = 0.0
        self.kf = KalmanFilter(dim_x=8, dim_z=4)
        self.kf.env = self.env
        self._init_kalman_filter()
    def _init_kalman_filter(self):
        self.kf._init_kalman_filter()
    def _state_to_bbox(self, state, frame_width, frame_height):
        state = np.nan_to_num(state, nan=0.0, posinf=1e6, neginf=-1e6)
        cx, cy, w, h, _, _, _, _ = state
        if w <= 0 or h <= 0 or np.any(np.isnan([w, h])):
            return None
        w = min(max(w, 1.0), frame_width)
        h = min(max(h, 1.0), frame_height)
        x_top_left = max(0, min(cx - w / 2, frame_width - w))
        y_top_left = max(0, min(cy - h / 2, frame_height - h))
        return [round(x_top_left, 1), round(y_top_left, 1), w, h]
    def update(self, action, detection=None, confidence=0.0, frame_width=1920, frame_height=1080):
        # self.logger.debug omitted for speed
        self.confidence = confidence
        action_str = ['a1', 'a2', 'a3', 'a4', 'a5'][action]
        if action_str in ['a3', 'a4', 'a5']:
            self.kf.predict()
            self.kf.x = np.nan_to_num(self.kf.x, nan=0.0, posinf=1e6, neginf=-1e6)
            self.kf.P = np.nan_to_num(self.kf.P, nan=0.0, posinf=1e6, neginf=-1e6)
        if action_str == 'a1':
            self.mode = 'inactive'
            self.bbox = None
            self.track_id = None
            self.kf.x = np.zeros(8)
            self.kf._init_kalman_filter()
            return True
        elif action_str == 'a2':
            if detection is None:
                return False
            if not isinstance(detection, (list, np.ndarray)) or len(detection) != 4:
                return False
            detection = np.nan_to_num(detection, nan=0.0, posinf=1e6, neginf=-1e6)
            if np.any(detection < 0):
                return False
            self.bbox = detection.copy()
            self.mode = 'visible'
            self.track_id = self.env.next_track_id if self.track_id is None else self.track_id
            self.env.next_track_id += 1
            x, y, w, h = detection
            w = min(max(w, 1.0), frame_width)
            h = min(max(h, 1.0), frame_height)
            self.bbox = [x, y, w, h]
            self.kf.x = np.array([x + w/2, y + h/2, w, h, 0, 0, 0, 0])
            self.kf._init_kalman_filter()
            return True
        elif action_str == 'a3':
            if detection is None:
                return False
            if not isinstance(detection, (list, np.ndarray)) or len(detection) != 4:
                return False
            detection = np.nan_to_num(detection, nan=0.0, posinf=1e6, neginf=-1e6)
            if np.any(detection < 0):
                return False
            if self.mode not in ['visible', 'hidden']:
                return False
            self.bbox = detection.copy()
            self.mode = 'visible'
            x, y, w, h = detection
            w = min(max(w, 1.0), frame_width)
            h = min(max(h, 1.0), frame_height)
            self.bbox = [x, y, w, h]
            z = np.array([x + w/2, y + h/2, w, h])
            self.kf.update(z)
            return True
        elif action_str in ['a4', 'a5']:
            if self.mode not in ['visible', 'hidden']:
                return False
            self.mode = 'visible' if action_str == 'a4' else 'hidden'
            cx, cy, w, h = self.kf.x[0:4]
            if w > 0.5 * frame_width or h > 0.5 * frame_height or np.isnan(w) or np.isnan(h):
                self._init_kalman_filter()
                self.bbox = None
                self.mode = 'inactive'
                return False
            self.bbox = self._state_to_bbox(self.kf.x, frame_width, frame_height)
            if self.bbox is None:
                self.mode = 'inactive'
                return False
            return True
        else:
            return False
    def reset(self):
        self.bbox = None
        self.mode = 'inactive'
        self.track_id = None
        self.n_unassoc = 0
        self.n_streak = 0
        self.confidence = 0.0
        self.kf.x = np.zeros(8)
        self.kf._init_kalman_filter()
    def get_observation(self, detection=None, confidence=0.0, cost=1.0, prior_x=None, frame_width=1920, frame_height=1080):
        # self.logger.debug omitted for speed
        obs = np.zeros(18)
        state = prior_x if prior_x is not None else self.kf.x
        state = np.nan_to_num(state, nan=0.0, posinf=1e6, neginf=-1e6)
        obs[0] = state[0] / frame_width
        obs[1] = state[1] / frame_height
        obs[2] = state[4] / frame_width
        obs[3] = state[5] / frame_height
        obs[4] = state[6] / frame_width
        obs[5] = state[7] / frame_height
        obs[6] = state[2] / frame_width
        obs[7] = state[3] / frame_height
        if detection is not None and not np.any(np.isnan(detection)):
            x, y, w, h = detection
            obs[8] = (x + w/2) / frame_width
            obs[9] = (y + h/2) / frame_height
            obs[10] = w / frame_width
            obs[11] = h / frame_height
        obs[12] = np.clip(confidence, 0, 1)
        obs[13] = (np.clip(cost, -1, 1) + 1) / 2
        mode_idx = ['inactive', 'visible', 'hidden'].index(self.mode)
        obs[14 + mode_idx] = 1.0
        obs[17] = 1 / (1 + np.exp(-self.n_unassoc))
        obs[16] = 1 / (1 + np.exp(-self.n_streak))
        obs = np.nan_to_num(obs, nan=0.0, posinf=1.0, neginf=0.0)
        obs = np.clip(obs, 0.0, 1.0)
        return obs
class MARLMOTEnv(gym.Env):
    logger = logging.getLogger(__name__)
    metadata = {'render_modes': []}
    def __init__(self, data_dir, ground_truth_dir, sequence="MOT17-05-SDP", max_agents=35):  # Reduced to 15
        super().__init__()
        self.data_dir = data_dir
        self.ground_truth_dir = ground_truth_dir
        self.sequence = sequence
        self.max_agents = max_agents
        self.frame_width = 1920
        self.frame_height = 1080
        self.detections = self.load_detections(data_dir, sequence)
        self.ground_truth = self.load_ground_truth(ground_truth_dir, sequence)
        self.total_frames = len(self.detections)
        self.T = self.total_frames
        self.total_g = sum(len(gt) for gt in self.ground_truth if gt)
        self.logger.debug(f"Loaded {self.total_frames} frames for sequence {sequence}")
        if self.total_frames == 0:
            self.logger.warning(f"Sequence {sequence} has no frames; setting to 1 dummy frame.")
            self.detections = [[]]
            self.ground_truth = [[]]
            self.total_frames = 1
            self.T = 1
            self.total_g = 0
        self._load_frame_size()
        self.agents = [Agent(self) for _ in range(self.max_agents)]
        self.frame_idx = 0
        self.next_track_id = 1
        self.episode_mota_components = {'m_t': 0, 'fp_t': 0, 'mme_t': 0, 'g_t': 0, 'matches': 0}
        self.prev_matches = {}
        self.current_d = None
        self.current_conf = None
        self.current_cost = None
        self.observation_space = Box(low=0.0, high=1.0, shape=(18,), dtype=np.float32)
        self.state_space = Box(low=0.0, high=1.0, shape=(self.max_agents * 18,), dtype=np.float32)
        self.action_space = MultiDiscrete([5] * self.max_agents)
        self.logger.info(f"Initialized MARLMOTEnv with {self.total_frames} frames, {self.max_agents} agents for sequence {sequence}")
        hom_path = os.path.join(self.data_dir, self.sequence, 'homographies.pkl')
        if os.path.exists(hom_path):
            self.homographies = pickle.load(open(hom_path, 'rb'))
        else:
            self.homographies = [np.eye(3)] * self.total_frames
            self.logger.warning(f"No homographies found for {self.sequence}, using identity.")
    def _load_frame_size(self):
        seqinfo_path = os.path.join(self.data_dir, self.sequence, 'seqinfo.ini')
        if os.path.exists(seqinfo_path):
            with open(seqinfo_path, 'r') as f:
                lines = f.readlines()
            for line in lines:
                if 'imWidth' in line:
                    self.frame_width = int(line.split('=')[1].strip())
                if 'imHeight' in line:
                    self.frame_height = int(line.split('=')[1].strip())
            self.logger.info(f"Loaded frame size: width={self.frame_width}, height={self.frame_height}")
        else:
            self.logger.warning(f"seqinfo.ini not found for {self.sequence}, using default 1920x1080")
    def load_detections(self, data_dir, sequence):
        det_path = os.path.join(data_dir, sequence, 'det', 'det.txt')
        if not os.path.exists(det_path):
            self.logger.warning(f"Detection file not found for {sequence}; using empty dataset.")
            return [[]]
        with open(det_path, 'r') as f:
            lines = [line.strip().split(',') for line in f if line.strip()]
        if not lines:
            self.logger.warning(f"Detection file {det_path} is empty; returning empty dataset.")
            return [[]]
        frame_ids = [int(line[0]) for line in lines]
        max_frame = max(frame_ids) if frame_ids else 1
        detections = [[] for _ in range(max_frame)]
        invalid_detections = 0
        for line in lines:
            frame_id = int(line[0]) - 1
            bbox = [float(x) for x in line[2:6]]
            if any(x < 0 for x in bbox):
                self.logger.warning(f"Negative coordinates in detection: {bbox}")
                bbox = [max(0, x) for x in bbox]
                invalid_detections += 1
            if bbox[2] <= 0 or bbox[3] <= 0:
                continue
            if 0 <= frame_id < len(detections):
                confidence = float(line[6]) if len(line) > 6 else 1.0
                detections[frame_id].append({'bbox': bbox, 'confidence': confidence})
            else:
                self.logger.warning(f"Invalid frame_id {frame_id} in detections for {sequence}")
        self.logger.info(f"Loaded {len(detections)} frames of detections for {sequence}, {invalid_detections} had negative coordinates")
        return detections
    def load_ground_truth(self, ground_truth_dir, sequence):
        gt_path = os.path.join(ground_truth_dir, sequence, 'gt', 'gt.txt')
        if not os.path.exists(gt_path):
            self.logger.warning(f"Ground truth file not found for {sequence}; using empty dataset.")
            return [[]]
        with open(gt_path, 'r') as f:
            lines = [line.strip().split(',') for line in f if line.strip()]
        if not lines:
            self.logger.warning(f"Ground truth file {gt_path} is empty; returning empty dataset.")
            return [[]]
        frame_ids = [int(line[0]) for line in lines]
        max_frame = max(frame_ids) if frame_ids else 1
        ground_truth = [[] for _ in range(max_frame)]
        for line in lines:
            frame_id = int(line[0]) - 1
            if 0 <= frame_id < len(ground_truth):
                obj_id = int(line[1])
                bbox = [float(x) for x in line[2:6]]
                visibility = float(line[8]) if len(line) > 8 else 1.0
                class_id = int(line[7]) if len(line) > 7 else 1
                if visibility > 0.0 and class_id == 1:
                    ground_truth[frame_id].append({'bbox': bbox, 'id': obj_id})
            else:
                self.logger.warning(f"Invalid frame_id {frame_id} in ground truth for {sequence}")
        self.logger.info(f"Loaded {len(ground_truth)} frames of ground truth for {sequence}")
        return ground_truth
    def reset(self, *, seed=None, options=None):
        # self.logger.debug omitted for speed
        for a in self.agents:
            a.reset()
        self.frame_idx = 0
        self.next_track_id = 1
        self.episode_mota_components = {'m_t': 0, 'fp_t': 0, 'mme_t': 0, 'g_t': 0, 'matches': 0}
        self.prev_matches = {}
        obs_list, state, self.current_d, self.current_conf, self.current_cost = self._get_obs_and_assigns()
        info = {'frame_idx': self.frame_idx}
        # self.logger.info omitted for speed
        return {'obs': obs_list, 'state': state}, info
    def _get_obs_and_assigns(self):
        # self.logger.debug omitted for speed
        if self.frame_idx >= len(self.detections):
            # self.logger.debug omitted
            return np.zeros((self.max_agents, 18)), np.zeros(self.max_agents * 18), [None]*self.max_agents, [0.0]*self.max_agents, [1.0]*self.max_agents
        detections = self.detections[self.frame_idx]
        prior_x_list = []
        prior_bbox_list = []
        for a in self.agents:
            if a.mode == 'inactive':
                prior_x = np.zeros(8)
                prior_bbox = [0, 0, 0, 0]
            else:
                prior_x = np.dot(a.kf.F, a.kf.x)
                prior_bbox = a._state_to_bbox(prior_x, self.frame_width, self.frame_height)
                if prior_bbox is None:
                    prior_bbox = [0, 0, 0, 0]
            prior_x_list.append(prior_x)
            prior_bbox_list.append(prior_bbox)
        det_bboxes = [d['bbox'] for d in detections]
        det_confs = [d['confidence'] for d in detections]
        agent_d = [None] * self.max_agents
        agent_conf = [0.0] * self.max_agents
        agent_cost = [0.0] * self.max_agents
        if det_bboxes:
            iou_matrix = compute_iou_vectorized(det_bboxes, prior_bbox_list)
            cost_matrix = -iou_matrix
            row_ind, col_ind = linear_sum_assignment(cost_matrix)
            for k in range(len(row_ind)):
                r = row_ind[k]
                c = col_ind[k]
                if c < self.max_agents:
                    agent_d[c] = det_bboxes[r]
                    agent_conf[c] = det_confs[r]
                    agent_cost[c] = cost_matrix[r, c]
                    # self.logger.debug omitted
                else:
                    # self.logger.debug omitted
                    pass
        for i, a in enumerate(self.agents):
            if a.mode in ['visible', 'hidden']:
                if agent_d[i] is not None:
                    a.n_streak += 1
                    a.n_unassoc = 0
                else:
                    a.n_unassoc += 1
                    a.n_streak = 0
            else:
                a.n_streak = 0
                a.n_unassoc = 0
        obs = [self.agents[i].get_observation(agent_d[i], agent_conf[i], agent_cost[i], prior_x=prior_x_list[i], frame_width=self.frame_width, frame_height=self.frame_height) for i in range(self.max_agents)]
        per_agent_obs = np.array(obs)
        global_state = per_agent_obs.flatten()
        return per_agent_obs, global_state, agent_d, agent_conf, agent_cost
    def compute_mota(self, predictions, ground_truth, update_matches=True):
        if not ground_truth:
            fp_t = sum(1 for p in predictions if p['mode'] == 'visible' and p['bbox'] is not None)
            return {'m_t': 0, 'fp_t': fp_t, 'mme_t': 0, 'g_t': 0, 'matches': 0}
        gt_bboxes = [gt['bbox'] for gt in ground_truth]
        gt_ids = [gt['id'] for gt in ground_truth]
        pred_bboxes = [p['bbox'] for p in predictions if p['mode'] == 'visible' and p['bbox'] is not None]
        pred_ids = [p['id'] for p in predictions if p['mode'] == 'visible' and p['bbox'] is not None]
        matches = []
        mme_t = 0
        if gt_bboxes and pred_bboxes:
            iou_matrix = compute_iou_vectorized(pred_bboxes, gt_bboxes)
            cost_matrix = 1.0 - iou_matrix
            row_ind, col_ind = linear_sum_assignment(cost_matrix)
            for ii, jj in zip(row_ind, col_ind):
                if cost_matrix[ii, jj] <= 0.5:
                    matches.append((ii, jj))
                    gt_id = gt_ids[jj]
                    track_id = pred_ids[ii]
                    if gt_id in self.prev_matches and self.prev_matches[gt_id] != track_id:
                        mme_t += 1
                    if update_matches:
                        self.prev_matches[gt_id] = track_id
        m_t = len(gt_bboxes) - len(matches)
        fp_t = len(pred_bboxes) - len(matches)
        g_t = len(gt_bboxes)
        self.logger.info(f"MOTA components: m_t={m_t}, fp_t={fp_t}, mme_t={mme_t}, g_t={g_t}, matches={len(matches)}")
        return {'m_t': m_t, 'fp_t': fp_t, 'mme_t': mme_t, 'g_t': g_t, 'matches': len(matches)}
    def step(self, action):
        # self.logger.debug omitted for speed
        if self.frame_idx >= len(self.detections):
            g_t = self.episode_mota_components['g_t']
            mota = 0.0 if g_t == 0 else 1 - (self.episode_mota_components['m_t'] + self.episode_mota_components['fp_t'] + self.episode_mota_components['mme_t']) / g_t
            self.logger.info(f"Episode done, MOTA={mota}")
            return {'obs': np.zeros((self.max_agents, 18)), 'state': np.zeros(self.max_agents * 18)}, 0.0, True, False, {'mota': mota}
        detections = self.detections[self.frame_idx] if self.frame_idx < len(self.detections) else []
        ground_truth = self.ground_truth[self.frame_idx] if self.frame_idx < len(self.ground_truth) else []
        self.logger.info(f"Frame {self.frame_idx}: {len(detections)} detections, {len(ground_truth)} ground truth objects")
        if len(detections) == 0 and len(ground_truth) == 0:
            self.frame_idx += 1
            obs_list, state, self.current_d, self.current_conf, self.current_cost = self._get_obs_and_assigns()
            self.logger.info(f"No detections or ground truth, returning reward=0")
            return {'obs': obs_list, 'state': state}, 0.0, False if self.frame_idx < len(self.detections) else True, False, {'mota': 0.0}
        for i in range(self.max_agents):
            d = self.current_d[i]
            conf = self.current_conf[i]
            self.agents[i].update(action[i], d, conf, self.frame_width, self.frame_height)
        predictions = [{'bbox': a.bbox, 'mode': a.mode, 'id': a.track_id} for a in self.agents]
        mota_components = self.compute_mota(predictions, ground_truth)
        # Improved reward: normalized by g_t, + match bonus
        errors = mota_components['m_t'] + mota_components['fp_t'] + mota_components['mme_t']
        g_t = mota_components['g_t']
        reward = -errors / max(1, g_t) + 0.1 * mota_components['matches']
        reward = -errors / max(1, g_t) 
        if mota_components['m_t'] > 0 or mota_components['fp_t'] > 0 or mota_components['mme_t'] > 0 or mota_components['matches'] > 0:
            self.logger.info(f"Non-zero reward frame: reward={reward}, matches={mota_components['matches']}, errors={errors}, objects={g_t}")
        self.logger.info(f"Sequence {self.sequence}, Frame {self.frame_idx + 1}: m_t={mota_components['m_t']}, fp_t={mota_components['fp_t']}, mme_t={mota_components['mme_t']}, g_t={g_t}, matches={mota_components['matches']}, num_visible={len([p for p in predictions if p['mode'] == 'visible'])}, reward={reward}")
        if reward == 0:
            self.logger.warning(f"Reward is 0: m_t={mota_components['m_t']}, fp_t={mota_components['fp_t']}, mme_t={mota_components['mme_t']}, g_t={g_t}")
        for key in mota_components:
            self.episode_mota_components[key] += mota_components[key]
        self.frame_idx += 1
        done = self.frame_idx >= len(self.detections)
        obs_list, state, self.current_d, self.current_conf, self.current_cost = self._get_obs_and_assigns()
        info = {'mota': 0.0 if self.episode_mota_components['g_t'] == 0 else 1 - (self.episode_mota_components['m_t'] + self.episode_mota_components['fp_t'] + self.episode_mota_components['mme_t']) / self.episode_mota_components['g_t']}
        # self.logger.info omitted for speed
        return {'obs': obs_list, 'state': state}, reward, done, False, info
    def render(self, mode='human'):
        pass
# QMIX Components
class QNetwork(nn.Module):
    def __init__(self, input_dim=18, hidden_dim=32, n_actions=5):  # Smaller hidden
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.dropout1 = nn.Dropout(0.1)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.dropout2 = nn.Dropout(0.1)
        self.fc3 = nn.Linear(hidden_dim, n_actions)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        return self.fc3(x)
class MixingNetwork(nn.Module):
    def __init__(self, n_agents=35, state_dim=35*18, mixing_dim=32):  # Adjusted for 15 agents
        super(MixingNetwork, self).__init__()
        self.n_agents = n_agents
        self.state_dim = state_dim
        self.mixing_dim = mixing_dim
        self.hyper_w_1 = nn.Linear(self.state_dim, n_agents * mixing_dim)
        self.hyper_b_1 = nn.Linear(self.state_dim, mixing_dim)
        self.hyper_w_final = nn.Linear(self.state_dim, mixing_dim)
        self.hyper_b_final = nn.Linear(self.state_dim, 1)
    def forward(self, agent_qs, state):
        bs = agent_qs.size(0)
        w1 = torch.abs(self.hyper_w_1(state)).view(bs, self.n_agents, self.mixing_dim)
        b1 = self.hyper_b_1(state)
        weighted_sum = (agent_qs.unsqueeze(-1) * w1).sum(dim=1)
        hidden = F.relu(weighted_sum + b1)
        w_final = torch.abs(self.hyper_w_final(state)).unsqueeze(-1)
        b_final = self.hyper_b_final(state)
        q_tot = torch.matmul(hidden, w_final).squeeze(-1) + b_final.squeeze(-1)
        return q_tot
class QMIXAgent:
    def __init__(self, agent_id, obs_dim=18, n_actions=5, lr=5e-5, device='cpu'):
        self.agent_id = agent_id
        self.q_net = QNetwork(obs_dim, n_actions=n_actions).to(device)
        self.target_q_net = copy.deepcopy(self.q_net).to(device)
        self.optimizer = torch.optim.RMSprop(self.q_net.parameters(), lr=lr)
        self.device = device
    def select_action(self, obs, epsilon=0.05):
        obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        self.q_net.eval()  # Eval for selection
        with torch.no_grad():
            q_values = self.q_net(obs)
        if random.random() < epsilon:
            return random.randint(0, 4)
        return q_values.max(1)[1].item()
class ReplayBuffer:
    def __init__(self, capacity=10000):  # Reduced capacity
        self.buffer = deque(maxlen=capacity)
    def push(self, state, obs, actions, reward, next_state, next_obs, done):
        self.buffer.append((state.copy(), obs.copy(), np.array(actions), float(reward), next_state.copy(), next_obs.copy(), float(done)))
    def sample(self, batch_size, device):
        batch = random.sample(self.buffer, batch_size)
        states, obs_list, acts, rws, next_states, next_obs_list, dns = zip(*batch)
        states = np.stack(states)
        obs = np.stack(obs_list)
        acts = np.stack(acts)
        rws = [r for r in rws]
        next_states = np.stack(next_states)
        next_obs = np.stack(next_obs_list)
        dns = [d for d in dns]
        return (torch.FloatTensor(states).to(device),
                torch.FloatTensor(obs).to(device),
                torch.LongTensor(acts).to(device),
                torch.FloatTensor(rws).unsqueeze(1).to(device),
                torch.FloatTensor(next_states).to(device),
                torch.FloatTensor(next_obs).to(device),
                torch.FloatTensor(dns).unsqueeze(1).to(device))
def create_marlmot_env(data_dir, ground_truth_dir, sequence, max_agents):
    return MARLMOTEnv(data_dir, ground_truth_dir, sequence, max_agents)
def train_with_qmix(mot17_dir, ground_truth_dir, sequences, max_agents=35, total_timesteps=1000000000000000000000):
    logger = logging.getLogger(__name__)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"Using device: {device}")
    valid_sequences = []
    for seq in sequences:
        det_path = os.path.join(mot17_dir, seq, 'det', 'det.txt')
        gt_path = os.path.join(ground_truth_dir, seq, 'gt', 'gt.txt')
        if os.path.exists(det_path) and os.path.exists(gt_path):
            valid_sequences.append(seq)
            logger.info(f"Found valid sequence: {seq}")
        else:
            logger.warning(f"Skipping {seq}: det.txt exists: {os.path.exists(det_path)}, gt.txt exists: {os.path.exists(gt_path)}")
    if not valid_sequences:
        logger.error("No valid sequences found; check your data directory.")
        raise ValueError("No valid sequences found; check your data directory.")
    train_seqs = valid_sequences[:-1] if len(valid_sequences) > 1 else valid_sequences
    val_seq = valid_sequences[-1] if len(valid_sequences) > 1 else valid_sequences[0]
    env_fn = partial(create_marlmot_env, mot17_dir, ground_truth_dir, train_seqs[0], max_agents)
    env = env_fn()
    agents = [QMIXAgent(i, obs_dim=18, n_actions=5, device=device) for i in range(max_agents)]
    agent_optimizers = [agent.optimizer for agent in agents]
    mixer = MixingNetwork(n_agents=max_agents, state_dim=max_agents*18).to(device)
    target_mixer = copy.deepcopy(mixer).to(device)
    mixer_optimizer = torch.optim.RMSprop(mixer.parameters(), lr=1e-5)
    replay_buffer = ReplayBuffer()
    epsilon = 1.0
    epsilon_decay = 0.9995  # Slower decay
    batch_size = 64  # Larger batch
    gamma = 0.95  # Lower gamma
    tau = 0.005
    state, info = env.reset()
    episode_reward = 0
    step_count = 0
    ep_rewards = []
    ep_motas = []
    best_mota = -np.inf
    pbar = tqdm(total=total_timesteps, desc="QMIX Training")
    while step_count < total_timesteps:
        obs = state['obs']
        global_state = state['state']
        actions = [agents[i].select_action(obs[i], epsilon) for i in range(max_agents)]
        next_state, reward, done, _, info = env.step(actions)
        replay_buffer.push(global_state, obs, actions, reward, next_state['state'], next_state['obs'], done)
        episode_reward += reward
        state = next_state
        step_count += 1
        epsilon = max(0.1, epsilon * epsilon_decay)
        print(epsilon)
        pbar.update(1)
        if len(replay_buffer.buffer) > batch_size and step_count % 4 == 0:
            states, obs_t, acts, rws, next_states, next_obs_t, dones = replay_buffer.sample(batch_size, device)
            # Train mode for updates
            for agent in agents:
                agent.q_net.train()
            with torch.no_grad():
                next_max_qs_list = [agents[i].target_q_net(next_obs_t[:, i, :]).max(1)[0] for i in range(max_agents)]
                next_max_qs = torch.stack(next_max_qs_list, dim=1)
                next_q_tot = target_mixer(next_max_qs, next_states)
                # Clip for overestimation
                next_q_tot = torch.clamp(next_q_tot, min=-1e4, max=0.0)
                targets = rws.squeeze(1) + gamma * next_q_tot * (1 - dones.squeeze(1))
            selected_qs_list = [agents[i].q_net(obs_t[:, i, :]).gather(1, acts[:, i].unsqueeze(1)).squeeze(1) for i in range(max_agents)]
            selected_qs = torch.stack(selected_qs_list, dim=1)
            q_tot = mixer(selected_qs, states)
            loss = F.mse_loss(q_tot, targets)
            for opt in agent_optimizers:
                opt.zero_grad()
            mixer_optimizer.zero_grad()
            loss.backward()
            for opt in agent_optimizers:
                opt.step()
            mixer_optimizer.step()
            # Soft updates
            for agent in agents:
                for target_param, local_param in zip(agent.target_q_net.parameters(), agent.q_net.parameters()):
                    target_param.data.copy_(tau * local_param.data + (1 - tau) * target_param.data)
            for target_param, local_param in zip(target_mixer.parameters(), mixer.parameters()):
                target_param.data.copy_(tau * local_param.data + (1 - tau) * target_param.data)
            # Save best model if improved
            if done:
                mota = info['mota']
                if mota > best_mota:
                    best_mota = mota
                    torch.save([agent.q_net.state_dict() for agent in agents], 'best_qmix_agents.pth')
                    torch.save(mixer.state_dict(), 'best_qmix_mixer.pth')
        if done:
            mota = info['mota']
            ep_rewards.append(episode_reward)
            ep_motas.append(mota)
            logger.info(f"Episode reward: {episode_reward}, MOTA: {mota}, Moving Avg MOTA: {np.mean(ep_motas[-10:]) if len(ep_motas)>=10 else mota}")
            if len(ep_rewards) % 10 == 0:
                combine_plot(ep_rewards, ep_motas)
            state, info = env.reset()
            episode_reward = 0
    env.close()
    torch.save([agent.q_net.state_dict() for agent in agents], 'qmix_agents.pth')
    torch.save(mixer.state_dict(), 'qmix_mixer.pth')
    torch.save(target_mixer.state_dict(), 'qmix_target_mixer.pth')
    logger.info("QMIX Training completed successfully")
    combine_plot(ep_rewards, ep_motas)
    return agents, mixer
def combine_plot(rewards, motas):
    if not rewards or not motas:
        logging.warning("No data to plot.")
        return
    episodes = range(1, len(rewards) + 1)
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)
    ax1.set_ylabel('Episode Reward')
    ax1.plot(episodes, rewards, color='tab:blue', marker='o', label='Reward')
    ax1.set_title('Episode Reward')
    ax2.set_xlabel('Episodes')
    ax2.set_ylabel('MOTA')
    ax2.plot(episodes, motas, color='tab:red', marker='x', label='MOTA')
    ax2.set_title('MOTA')
    fig.suptitle('QMIX Training Metrics')
    fig.tight_layout()
    plt.savefig('qmix_combined_training_plot.png')
    plt.close(fig)  # No show, no block
def main():
    mot17_dir = r"D:\An\tracking\train\train"
    ground_truth_dir = mot17_dir
    sequences = ['MOT17-05-SDP']
    logger = logging.getLogger(__name__)
    logger.info("Starting QMIX training")
    agents, mixer = train_with_qmix(mot17_dir, ground_truth_dir, sequences, max_agents=35, total_timesteps=1000000000000000000000000)
    logger.info("QMIX Training completed. Check 'marlmot_training_detailed.log' for logs.")
if __name__ == "__main__":
    main()
